## Inode-FS
- ### Layers
	- Block Layer:
		- 4KB Size per block (maybe but adjustable)
		- Contains bitmap to track the usability of the block
		  id:: 654f666b-a243-49f6-8077-f1324f3092b4
	- File Layer:
		- **Representative**: Inode
		- Resides in the block too. Holds the location where real data stores and the file's **metadata**.
		- ![image.png](../assets/image_1699702663883_0.png)
		- Indirect holds another map to block which holds other indirect blocks
		- Only the direct block holds the real data block which stores the actual data
	- Inode number layer
		- Just an array stores behind the block bitmap's with also inode's bitmap.
		- **Note that: in modern ext4, inode table has actual inode inside instead of the block number, this is different to lab assignment design**
		- Indicates the block number of the inode layer
		- ![image.png](../assets/image_1699895099342_0.png)
		-
	- File Name Layer
		- Hide metadata of file management
		- Separate the directory type and regular file type
		- Holds the mapping from the filename to the inode number.
	- Path Name Layer
		- Recursively look up the directory inode untils it lands on the regular files.
		- The inode number of the root directory is fixed. In the case of the ext4, it's assigned with 1. Note that we don't have number 0 here, since it represents the dysfunction of the file system.
	- ### Links
		- Two semantic abstractions
			- LINK (from, to) --- to's name to from's inode
			- UNLINK (name) --- decrease the reference count of the inode where name points to.
		- Create Shortcut from one link to another.
		- We store the reference count inside the inode.
		- We increment the refcnt of the inode if one link is formed. The default is always one.
		- When the reference count of the inode reaches zero, we can safely remove the inode and free the data blocks.
		- Note that soft link doesn't increment the reference counting.
	- #### Renaming(from,to)
		- When renaming, it should be guaranteed that LINK(from, to) is done atomically and should not fail.
	- #### Two Types of lINKS
		- Soft Link --- only associates the destination with the name
		- Hard Link --- associates the destination with the inode number
	-
- ## FAT System
	- **Only Two Layers**
		- File associated entry
			- starting from the root and finds the first block of the backed file and traverse the linked list.
		- Free List
			- contains the free block number and perform allocation and garbage collecting based on this free list.
		- ![image.png](../assets/image_1699791405135_0.png)
		- Linked List Structure
			- Point to the end of the block list when ended.
		- Directory
			- Hold the next file entry until it reaches the end.
			- Hold the metadata of the file and the entry of another file/directory.
			- ![image.png](../assets/image_1699792373187_0.png)
		- Advantage:
			- Naive and simple --- used as the supporting file system for efi when designing the UEFI boot system
- ## File System Abstraction
	- ### Open
		- Gives the process a fd in its own fd table which points to an entry in the file table
		- Each process has its own fd table but shares the file entry table on the OS level
		- One open increase the reference count of the file table entry and the file itself,  each child process will increase also
		- However, another open will get to another entry separating the file cursor apart from each other
	- ### Close
		- Remove the fd table entry from the process
		- Decrement the reference count of the file table entry
		- Remove the file table entry if the reference count reaches zero
	- ### Read
		- Move the file cursor and read the data block.
		- Update the **Access** time (which means write operation )of itself and its all ancestors recursively (Can be disabled via system-level noatime mount)
	- ### Write/Append (No need to mention here)
	- ### Sync
		- Flush the page cache to the disk
		- Ensure that all the content is flushed into the disk on time or on recovery whatever it may happen.
	-
- ## RPC
	- > RPC Stands for Remote Procedure Call which provides an easy-to-use executional and functional abstraction without coding the details for the remote interaction.
	- ### STUB
		- STUB Translate the function call into the internet message and transmit the requests and responses accordingly over the network ports.
		- Responsibility:
			- ![image.png](../assets/image_1699792931192_0.png)
		- #### Request Message
		- ![image.png](../assets/image_1699792982844_0.png)
		- #### Response Message
		- ![image.png](../assets/image_1699793021025_0.png)
	- ### Parameter Passing
		- Parameter Passing needs a machine-independent, language independent byte encoding.
		- Two types of format representations
			- Human Friendly: more verbose but needs more bits to store
			- Machine Friendly: less verbose but needs less bits to store
	- ### Failure Recovery
		- Network Failure can happen when on requesting, server transmission and even responding.
		- Requests resending is not tolerable since a lot of transaction is not Idempotent
		- We need semantics that can support that only zero or one request can be sent to the server for responding.
	-
- ## Network File System
- ### NFS
	- We don't have open and close here We only get a mount operation which returns a file handler to the client.
	- #### File Handler
		- File System Identifier
		- Inode Number -- Dealing with renaming
		- Generation Number  -- Versioning (in case of deletion and reallocation happens)
	- #### Performance
		- We need caching on client sides to get ride of excessive read,readlink,getattr,lookup,readdir
		- We need cache binding, metadata and directory binding
		- Dealing With Cache Coherence
			- Type 1: Guarantee read/write coherence on every operation or simply for some kinds of operations
			- Type 2:  Close-to-Open Consistency (we need to perform getattr when opening and flushing all writes when closing)
- ### VFS
	- In memory abstraction over different implmentations
	- Provides a highly abstracted set of APIs including OPEN, READ, WRITE, CLOSE
	- Even FS with no real local files supported can be abstracted this way (Proc FS)
	- #### Validation
		- Both server and client save timestamp of files
		- We always need to validate the consistency between the client and the server
		- Data Block should be flushed on close if possible.
		- ![image.png](../assets/image_1699796315525_0.png)
		-
	- #### Performance Improvment
	- ![image.png](../assets/image_1699796366885_0.png)
- ### GFS (Compared to the naive design in the CHFS)
	- #### Distributed Block Layer
		- Data Blocks are distributed across multiple data-server
		- Inode table is stored on the master server and path name resolution on the metadata server.
	- #### Interfaces
		- Only Create/Delete/Open/Close/Read/Write
		- Additional: Snapshot/Append
		- Unupported Ops: Link, Symlink, Rename
	- #### Architecture
		- Master + N Replicated Chuck Servers
		  Features:
			- Large Chunk  (64MB)
				- Reduce the need for frequent communication with master for chunk location info
				- Make it feasible to keep a TCP connection open for a extended time
				- Master Stores all metadata in memory
			- Master:
				- Maintains the metadata of the whole file system including the locations of the chunk and filename -chunk mappings.
				- Store all the data in the memory and persist an operation log
				- Chunk Location is not stored persistently for consistency management
	- #### Client
		- No OS Level API (No syscall / trap)
		- Interacts with master and chunk server directly
		- No caching
	- Operations:
		- ![image.png](../assets/image_1699884328280_0.png)
		- ![image.png](../assets/image_1699884281504_0.png)
		- ![image.png](../assets/image_1699884289029_0.png)
		- ![image.png](../assets/image_1699884296042_0.png)
		-
- ## Consistency Models
- ### Strict Consistency
	- Coherent to the global time no compromise.
	- Operation happens one by one and no overlaps
- ### Sequential Consistency
	- Consistent issue to complete in a single program in one machine
	- No operation reordered due to network latency with in that program
- ### Linearizability
	- If operation is issued, it must be able to see to the changes happened before it's issued
	- If certain operations overlaps from one another, the consistency is relaxed.
- #### Implementation
	- We use a sequence number just like TCP to reorder the write operation
	- Read Operation Can be relaxed by local caching so we can read the cache.
- ### Causal Consistency
	- Causal Consistency is a higher level of consistency meaning that one operation should have an effect on the sequential order of operations. We don't want some conversation with an important context that goes out of order.
- #### Implementation
	- Ordered operation log --- log the operation before performing it and reorder the log based on some order --- Time
	- We issue the <Time T,  Node ID>  to reorder the logging and to use as a tiebreaker if timestamp comes to a tie.
	- Step by Step clock synchronization --- Lamport Clock
		- When srv receives the message <T', srv'> it compares its own T with T' and updates T atomically with max (T, T'+1)
			- Derivation: vector clock if we want partial clock order if we want to pack two streams of events instead only single one.
			- Now the clock becomes [T1, T2, ..., Tn] we updates on clock when we receives nth event's update Tn = max(Tn, Tn' +1)
	- Log Truncation
		- Attach a sequence number to each transaction and discard all the logs with smaller sequence number.
- ## All or Nothing? Logging
- ### Redo Logging
	- Collecting all the changes and persist the log before hand. (We need to ensure the congruity of the log file we persist we need to add checksum to the log we append.)
	- Update the memory accordingly.
	- Recovery just traverse all the entries and perform state transition one by one.
	- **Pros**
		- Efficient logging. Only one single append operation.
	- **Cons**
		- Waste of disk I/O
		- Need but buffer every update until commit
		- Log File is continously growing. No truncation too much space.
- ### Undo-Redo Logging
	- We persist the operation into logging before performing each operation
		- Tx ID
		- Operation ID
		- Pointer to the previous record in the transaction
		- Value (file name, offset, value diff)
	- Append a commit sign with a pointer to the last undo log in the transaction at the end of the transaction or action.
	- When we recover, we traverse from the end of log and regroup each transaction starting from the commit bit  at the end. For redo log entry that is obsolete and orphan, we undo the changes. For those that are identified in the commit, we redo them.
- ### Checkpointing
	- Wait until no actions are in progress.
	- Trim the logging file to save space
		- Logging the ongoing transaction ID and their logs into the checkpoint and flush the page cache.
		- Discard the logging that's commited before the checkpoint happens.
		- When crashed, traverse the recent checkpoint and perform the redo-undo recovery
	- ![image.png](../assets/image_1699854179484_0.png)
- ## Core of transaction and block atomicity -- 2PL and OCC
	- We need to ensure that each transaction is somehow atomic that can be disturbed by other transaction even if they share some part of data in the database.
	- There are two ways achieving this: either pessimistically or optimistically
- ### Serializability
- #### Conflict Serializability
	- > A schedule is conflict serializable if the order of its conflicts (the order in
	  which the conflicting operations occur) is the same as the order of
	  conflicts in some sequential schedule
	- Conflict means that two different transaction has at least one write operation on the shared data.
	- Each transaction is viewed as a single node in the conflict graph. Connecting one node to another means that at this sequential moment, the operation that is conflict happened before the second one.
	- ![image.png](../assets/image_1699854810319_0.png)
	- #### View Serializability
	- This is a related model meaning that the scheduling and execution of each operation in each transaction will result in the same intermediate view for every variable when performing reading in the single transaction  in the serial execution of each transaction.
	- #### Final State Serializability
	- This is the most flexible one meaning that only the final state of the transactions is all we care. We don't care what happened inside the operations scheduling.
- ### Pessimistic: Two Phase Locking
	- Principles
		- Fine-grained lock control on some portion of the data.
		- Acquire the lock in the same order even if in different transaction.
		- Avoid Dead Lock Or Detect the Dead Lock in a heuristic manner
		- The transaction can not reacquire the lock after releasing it.
		- Stricter 2PL requires that the release of the lock should happen at the end of transaction. But normal ones don't
- ### Optimistic: OCC / MVCC
	- ![image.png](../assets/image_1699938662266_0.png)
	- ![image.png](../assets/image_1699938672422_0.png)
	- #### OCC  Steps
		- Collect the read sets and write sets.
		  logseq.order-list-type:: number
		- Lock each element in the write set.
		  logseq.order-list-type:: number
		- Read the element in the read set and Abort the transaction if and only if d is changed when reading or has been locked by other transactions (Check of read/write conflict happened)
		  logseq.order-list-type:: number
		- Write the element in the write set.
		  logseq.order-list-type:: number
		- Release all the lock.
		  logseq.order-list-type:: number
	- #### Advantages
		- Read-intensive applications can benefit because reading doesn't need to acquire locking.
	- #### Disadvantages
		- A large portion of abort operations happen when transactions rise
	- Note that the write_and_validate semantics can happen any time regardless of operation execution order. The execution only matters for the read_set state so it may cause false abort. Meaning that although the execution order is correct for serializability, but still got aborted. The execution order only affects when the read/write sets are collected, but doesn't guarantee when it is committed.
	- ### Locking Semantics
		- Compare and Swap
		  logseq.order-list-type:: number
		- Fetch And Add (ticket lock)
		  logseq.order-list-type:: number
- ### The Most Usable One -- MVCC
	- ![image.png](../assets/image_1699938704144_0.png)
	- #### Principles
		- Each read operation gets the most recent snapshot of the data.
		- Each write operation append a versioning number into the data columns.
	- #### Implementation
		- We need to add new version when writing.
		- For reading, we don't need any validation now.
		- Others are merely the same with the OCC.
	- #### Disadvantage
		- This cannot eliminate the read/write conflict.
		  logseq.order-list-type:: number
		- We assume this happens really rarely. So we just makes the snapshot isolation of it. 
		  logseq.order-list-type:: number
- ## Multi-site Consistency (Primary To Backup)
	- ### Two Phase Commit
		- #### Higher Level Commit
			- Same transaction but needs to be replicated across different machines.
			- The responsibility of the coordinator to check whether the commit condition can meet across various machines.
		- #### Lower Level Commit
			- Multi transaction on the local machine.
		- Higher-layer transaction coordinates the execution of lower layer TXs.
		- Lower-layer transaction we needs turn the commit log into prepared log and only after the coordinator tells the machine that it can commit, then it performs the commit operation.
		- Lower-layer commit log contains a **extra reference to higher level TX number**, if failure happens, the slaves must ask the coordinators whether the commit can be done.
		- Higher-layer logs the commit operation instead of the lower-layer.
	- ### Semantics
		- #### Prepare
			- Send the transaction data from coordinator to all of its slaves and wait for PREPARE ack.
			- This is relatively the same as the GFS.
			- If prepare timeout, abort the current transactions on every machine.
		- #### Commit
			- After receiving all the prepare flags by the slaves, the commit can be emitted and transmitted to the slaves.
			- Coordinator must log the decision before sending in case that it fails after the preparation.
			- Coordinator might fail, therefore we need to think up another way to tolerate this failure.
- ## Replica Consistency (Availability Guarantee)
- ### Replicated State Machines (Linearizability)
	- Replicas revolves around the replicated state machines which made up by various log entries.
	- We need to sync and maintain a consensus on the log entries so there's only one single copy
	- There's only one copy of the world state now since we only have one copy.
	- Operations should start at the same position and  make state transition based on the log.
	- ![image.png](../assets/image_1699884099097_0.png)
	-
- ### Network Partitions and View Server
	- There may be  multiple coordinators in the networks but somehow partitioned by the network.
	- Coordinators need some assistance to decide whether which server is the primary.
	- The Principle is that only primary server can receive requests and forward the requests to all its backup.
	- Only if the primary server can get all ACK from **all its backup**, it then can respond safely.
	- Backup must rejects any requests before it was identified as the primary by the view server.
	- View server needs to ping the primary and when the primary fails, need to allocate another view to update.
- ![image.png](../assets/image_1699882387290_0.png)
- Even when partition happens between VS and S1
- ![image.png](../assets/image_1699882544360_0.png)
- When S1 and VS parition is removed, The S1 will hear about the new View 2  and asks to sync from S2 and act as the backup server.
- However, the view server may still needs some replicas therefore we need to decide it pessimistically with paxos.
- ### Single-decree Paxos (Decide one single log entry value)
- This method is to resolve the dilemma issue where the coordinator fails to restart in a really long time.
- This method is performed in a distributed manner meaning that there's no central coordinator whatsoever.
- It can only decide a **single value in the single location and remains immutable**.
- #### Semantics (On Position N)
	- Propose(ID)
		- The proposer chooses a SN and sends them to a list of acceptors.
		- The acceptors should be in at least the majority of the servers.
	- Promise(ID, Accepted_ID, Accepted_Value)
		- The Acceptor views the proposed ID, and check
			- If Current Accepted_ID > ID reject.
			- If Current Accepted ID < ID then
				- Log (ID) in the entry before making promise
				- Send Promise(ID, Accepted_ID, Accepted_Value) to the proposer, where Accepted_ID is the last accepted ID and value is its corresponding value.
	- Accept(ID, Accepted_Value)
		- If the proposer receives the majority OK from the acceptors.
		- It should
			- If one of the promises contains the accepted_value
				- choose the accepted_value with biggest large accepted_id and propogate them to the acceptors
			- Or choose its own value and propagate it to the accepters.
	- Accept_OK:
		- When receiving a new Accept, it validates its ID again
		- If success: Log(ID, Accepted_ID, Value) and send the Accept_OK back to proposer
	- Decide:
		- When the proposer receives the majority of Accept OK, then it logs (ID, Accepted_ID, Value) to its entry and send Decide(ID, Value) to the accepters.
- ### Mutli-paxos
- We may want to hold a consensus on a sequence of values e.g. log entries.
- The proposer now have a modified semantics where he can select the position that the system needs to reach a consensus on.
- When the conflict happens, it should select a bigger number and restart the round and write through the log entry.
- ![image.png](../assets/image_1699881013821_0.png)
- To optimize the paxos's performance, it's easy to think that only the leader can make proposal and batch all messages to reduce the overtime of request roundtrips.
- ---
- ## Raft
- We need to figure out a way to replicate the log quickly, notice that in a fully decentralized distributed method like PAXOS.
- Raft is a leader based consensus industry-ready algorithm for balancing both efficiency and correctness.
- ### Leader Election
- #### Invariant
- Leader in one partition always has the latest log entries and has the highest term index. Term decides whether this leader is still up-to-date and the log entry term index has the final say when the term is tied.
- Only one leader can be elected in one partition at one time.
- #### Practice
- So in candidate mode, the leader must send its proposed term and its latest log entry metadata to the followers or other candidates.
- If one of the follower or candidate or an old leader from other partition decides to vote, it should change its state to follower immediately.
- If the RequestVoteRPC fails, the candidate becomes the follower immediately.
- If the leader cannot collect enough votes (a simple majority), election should be reopened immediately.
- ### Log Replication
- #### Invariant
- Only **commited** log can not be overwritten.
- The commited log should be replicated to the majority of the partition. So the new elected leader should at least have all the up-to-date commited log (if not it will simply be rejected.)
- #### Practice
- Therefore, if one follower hasn't kept up with the leader, the leader should first replicate the missing or overwrite the missing entries for the follower to keep up with the leader. (That's why there's next index and last index for the leader to take notes.)
- ### Persistent Data
- CurrentTerm: it should be persisted in case that the term mismatched for the log and for the election stage,  the follower might vote twice if it dies and restart.
  logseq.order-list-type:: number
- Voted For: this is easy, in one term, one follower can only vote for one guy. Or in some cases, two leaders will emerge. 
  logseq.order-list-type:: number
- Log: Simple and understandable
  logseq.order-list-type:: number
- ---
- ## Networking
- ### Networking Layers
- #### Application (Something like html)
- #### Transport Layer
- **The Case of TCP**
- Three times of handshaking with seq, seq ack, ack with both of clients maintaining a sequence number.
- ![image.png](../assets/image_1704464336722_0.png)
- ![image.png](../assets/image_1704464372977_0.png)
- #### Network Layer
- ![image.png](../assets/image_1704464424722_0.png)
- Best efforts to deliver the data.
- IP Layer uses a routing table to route the source to another destination and jump between the routers to get to the destination.
- For a router, there's a control plane and a data plane. Control Plane adjust and calculate the forwarding table in real-time, Data time just deliberately forward the every incoming packet to the destination using the forwarding table.
- #### Link Layers
- #### Physical Transmission
- Parallel Transportation :
- Data Ports -- sending the data to the destination
  logseq.order-list-type:: number
- Ready bit - Indicating that the data is ready for the receiver to synchronize
  logseq.order-list-type:: number
- ACK bit -- receiver indicates that it has done with current receiving
  logseq.order-list-type:: number
- The time to send even one bit: 2 * $\delta t$, so the maximum rate is $1/(2* \delta t)$
- Shortcomings: when the data bits has a lot in parallel, the propagation time is just too long. The maximum data it can hold is too low
- Serial Transportation:
- Sending a bit one at a time at extremely high clock rate.
- Two things to settle here.
- How to synchronize the clock: using the VCO to lock the frequency when the voltage is frequently oscilating but can not be precise if the voltage is stable -> solving: using manchester code to convert 0->01 1->10 so that the rising edge infers 0 and dying edge infers 1.
- #### Multiplexing
- ![image.png](../assets/image_1704465936071_0.png)
- Steps:
- Divide a minimum frames required per second into time unit like 1/8000 = 125 us
  logseq.order-list-type:: number
- Multiply it with the transportation rate to get the maximum bits transported in the time unit 45M * 1/8000 = 5624bits
  logseq.order-list-type:: number
- Devide the time unit transporation rate by the frame size 5624/8 = 703
  logseq.order-list-type:: number
- If there are more connections than capacity, the rest of those are blocked.
  logseq.order-list-type:: number
- In Asynchronous approaches, the data is fragmented into large pieces instead of small pieces and delivered across the network via IP protocol. Multiplexed with queue and demultiplexed using the provided header.
- ![image.png](../assets/image_1704466672492_0.png)
- When demultiplexing, to distinguish where a frame begins, we reformat the data.
- #### Error Handling
- Router detects the error by force and will try to error correcting the data bit with best efforts.
  logseq.order-list-type:: number
- If error correction fails, the router will ask the sender to resend the data regardless of the protocol layer requires.
  logseq.order-list-type:: number
- we use  the hamming code to detect the failure of the transportation.
  logseq.order-list-type:: number
- ---
- ## IP Routing
- ### Dijkstra Approach
- Dijkstra approach uses flooding approach to advertise its part of topology knowledge to all the counterparts in the network.
- After all nodes in the network exchanges the topology, each node runs the dijkstra's algorithm independently to get the routing table and hopping decision.
- **Advantages**:  only works a small scale of network, Really resilient on fault.
- **Disadvantages**: incapable of scaling on large network because of excessive network traffic. Large Overhead.
- ###  Distance Vector + Bellman Ford
- Steps: We know that on a graph with only positive nodes, a shortest path from $u$ to $v$ can at most have $|V| -1$ nodes.
- So in every round, one node only advertises the forwarding table to its neighbor and each node do relaxation on path on each round.
- However, each node can only forward its forwarding table rows to its neighbor when the hop action in that specific row can not be forwarded to its neighbor (in the case of split horizon problem)
- ![image.png](../assets/image_1704527011267_0.png)
- in this case, A cannot forward to B about its routing to C because A also needs to hop to B to get to C.
- **Advantages**: low Overhead
- **Disadvantages**: Can not converge immediately. Prone to split horizon problem.
- ### How to Scale?
- Path Vector:  advertisement should include the path to accelerate the convergence.
- Hierachy of Routing: Route between regions and then with a region
- Topological Addressing: Assign addresses in contiguous blocks to make advertisements smaller.w
- #### Path Vector
- Send a table of its known path to every destination to its neighbor.
- Like Distance-Vector but include the full path in the routing advertisements.
- Overhead increases but convergence time decreases.
- The node rejects the path when the path contains itself.
- It will always choose the best path advertised by its neighbors.
- Discard any path that a neighbor stops advertising to.
- #### Hierachical Address Assignment & Routing
- Routing Table has two parts on forwarding table, Region aware and Local-aware.
- Hierachical Address Assignment across the regions when the router routes to the region first and in the region we have local routing tables.
- IP Address is now bound to the location right now.
- Hierachy brings complexity:
	- Address is bound to a location
	- Path may no longer be the shortest possible.
	- Algorithm has less detailed information
- More about hierarchy
	- Can extend to more levels
	- Different places can have different levels
- We need one routing protocol to route across regions and a different protocol to route within regions.
- Implies that there are devices on the edge of each region that can translate between protocols.
- #### Topological Addressing
- Each region in one region is assigned with contiguous addresses in the form of block and they can be specified succinctly via a particular notation.
- This keep advertisements small.
- ---
- ## Network Forwarding Interface
- ![image.png](../assets/image_1704528506090_0.png)
- Forwarding an IP Packet we need to forward the header and rewrite the source of our own.
- TTL should be decremented if it reaches zero the router drops it immediately.
- ![image.png](../assets/image_1704528673109_0.png)
- ---
- ## NAT
- A NAT router converts its exported port to the internal network's ip + port.
- So now the internal ip is mapped to the port of the router exports to the outer network.
- When NAT happens , the source IP will change to the router's IP.
- ---
- ## Ethernet
- In local network, the hub and switch links the machines in the local network.
- A network hub broadcasts the message whereas a switch exchanges peer-to-peer messages and other nodes can not spoof the messages.
- In ethernet, the router gets to the destination by identifying the Mac Address.
- In ethernet, if the workstation wants to send a packet:
	- Look up in its table to find whether the destination is in the local network.
	  logseq.order-list-type:: number
	- If yes, just send it to the corresponding port of the destintation in the local network.
	  logseq.order-list-type:: number
	- If not, sends it to the router and hoping that the router will successfully forward it to its proposed destination.
	  logseq.order-list-type:: number
- ### ARP Protocol
- Address Resolution Protocol decides where the destincation MAC's  in the local network.
- If the destination is not in the local network, the destination will be default to the router in the ARP protocol.
- In ARP, reply can also change the routing table in the node.
- ---
- ### Timeout Restoration
- We cannot use a fixed time to decide whether a timeout happens on the end point.
- Using a fixed timer will result in countless flood of retrying.
- Fixed timer will make every activity overlapped significantly killing the system's efficiency.
- Timeout is introduced by the at least once delivery promise.
- #### Adaptive Timer
- We continuously measures the RTT of the request to adjust the timeout period or uses the exponential back-off to adjust the timer.
- #### NAK Protocol
- Negative Acknowledgment means the receiver side should send a message that lists missing items.
- Receiver can count arriving segments rather than timer.
- Sender can have no timer.
- ---
- ### Duplication Suppression
- Since each packet has one nounce identifier, the receiver can maintain a table taking notes of the every nonce the receiver receives.
- However, the table might grow too large to handle. One optimization, the receiver can choose to select the biggest nouce to store the nouce number. Now the old nonce is an tombstone that can not be remove d.
- Use a different port for each port to let the receiver knows that the port is different so that different nonce is distributed.
- ---
- ### Sliding Window
- Advance on the lowest acknowledgement number received.
  logseq.order-list-type:: number
- Sliding on the window size given by the receiver.
  logseq.order-list-type:: number
- When the sender can slide the window with the continguous region starting from the lowest acknowledgement number in the current window, it can advance the window.
  logseq.order-list-type:: number
- For Performance, Window Size >= RTT  * bottleneck data rate to maximize the performance.
- ---
- ## Congestion Control
- Packets will congest since one path in the network has its maximum bound.
- If the packets overflow the router, router will simply drop the network packet.
- In this way, the router should come up with a mechanism to control the window size so that it can balance out the congestion and reach the best performance possible
- window size <= min( Receiver Buffer, RTT * bottleneck data rate)
- ### AIMD
- For Every RTT:
- No drop: cwnd = cwnd + 1
- A drop: cwnd = cwnd / 2  (duplicated ACK / Drop Rate)
- Drawbacks:
	- Not tolerable in wireless environment (unreliable) since in a sense we need to increase the data rate for best delivery.
	- Can produce better equality between two participants in the network.
- ![image.png](../assets/image_1704550326968_0.png)
- ---
- ## Naming
- ### DNS
- A root server mapping website names to ip
-
- ### Benefits of Hierachical Design
- Each zone is only responsible for a small portion of the networks
- ### DNS Good Points
- No need to specific a context
- DNS has no trouble generating unique names
- The name can also be user-friendly
- Very Scalable in simplicity, caching and delagations.
- ### Bad Design of DNS
- Policy: who should control the root zone
- Significant load on root servers: Many queries for non-existent names becomes a DOS attack
- Security: How to know that the IP is correct and changing of the mapping is correct.
-
- ![image.png](../assets/image_1704553082807_0.png)
- ![image.png](../assets/image_1704553104432_0.png)
- ---
- ## P2P
- Sometimes we need a decentralized environment
- ![image.png](../assets/image_1704615394142_0.png)
- In a sense, we need to preserve privacy and high fault tolerance in the cost of performance.
- ### BitTorrent
- Three roles:
- Tracker: announce the information about how many peers are online in the public network.
  logseq.order-list-type:: number
- Seeder: the participants that own the whole portion of the file.
  logseq.order-list-type:: number
- Peer: the participants that only hold part of the file and need to retrieve from the seeder or other peers.
  logseq.order-list-type:: number
- Torrent File consists of the tracker, the data length, the piece length and the checksum of each piece/
- #### Principle
- Random start to download the first chunk.
  logseq.order-list-type:: number
- Download the least popular piece concurrently first.
  logseq.order-list-type:: number
- In the last piece, parallel downloading and finish the first.
  logseq.order-list-type:: number
- ### DHT
- #### Principle
- The distributed key value look up is for one key storing at one designated machine in the public network.
- So each key and the machine address should be hashed into one uniform hash space (ring).
- Each key is stored in the successor in the hash space corresponding to the machine.
- For storage, each machine holds a distributed lookup table for documenting the next r nodes in the hash space and the binary lifting table on the 1/2 ahead 1/4 ahead 1/8 ahead and so on.
- For each lookup, we jump to the closest node based on the lifting table if that node is still alive.
- If not, we jump to the next r nodes hoping that those nodes can have reachable candidates when calling the lifting nodes.
- The amortized cost of getting and putting is now $O(log n)$
- When a node decides to join, it asks its closet reachable successor to brought the keys to the local and rejoin the network and ask its closet ancestor to update its r nodes and lifting table.
- This is down by partial broadcast.
- ### BlockChain Bitcoin
- Distributed ledger instead.
- The miner collects the ledger and transaction record and adds a random number to it hoping that the transaction hash will yield a dedicated pattern (e.g. 18-leading-zero)
- If the currency is exhausted, now the miner will have a gas fee when doing transaction.
- ---
- ## Distributed Training
- ### Parallelism Revisited
- #### Single core approach
- Instruction Pipelining --- Simple Strategy discussed in the ICS (Cannot be too long since the latency will be sky high)
- Super Scalar --- instruction level parallism (Still there are instruction-level dependency. Not very feasible to scale too)
- Mounting the clock speed is not feasible since the voltage will be too high.
- #### Multi-core Approach
- #### Multi-core CPU
- Pros: real concurrency instead of instruction level parallelism
- Cons: Cache Coherency Constraint.
- #### SIMD
- Single instruction multi data-stream approach
- We add a lot of ALU for the processing unit and hoping that adding vector will bring locality into places.
- However, the loading still costs a lot and prefetching is not feasible for branching device.
- When doing calculation, the general boost of the calculation can not  match the memory bandwidth.
- #### Roofline Model
- ![image.png](../assets/image_1704618260727_0.png)
- #### CUDA
- In cuda, because there are no such things called branching in order to squeeze the performance of SIMD, the regular branching will be substituted by a costly approach called masking.
- Simply, masking is masking those units that don't participate in calculation but still needs extra overhead.
- It decreases the branching and every unit now has one single program counter.
- ![image.png](../assets/image_1704618410307_0.png)
- We exploited on one single machine but simply it's not enough.
- CUDA still needs to provide development kit to the end user and there are trends for domain-specific accelerators to pop off for doing things that matrix multiplication.
- ### Multi-machine Concurrency
- #### Map-Reduce
- Discussed in chp not gonna focus more.
- Something to be noticed here.
- **Fault Tolerance**:
	- Worker: Each operation is idempotent simply by reexecution and redispatching the differences.
	- Master: Checkpointing before distributing the tasks.
	- GFS: fault tolerant service
	- Skipping bad records
- **Locality**: manually scheduling the tasks to the nodes which held the input chunks. Nodes are abstracted upon the chunkservers in GFS.
- **Refinement**: parallel execution distributing the tasks to several nodes together
- ![image.png](../assets/image_1704619270291_0.png)
- #### Computation Graph
- Describing the chain of calculation in the graph through DAG. Each node describes the dependency.
- ![image.png](../assets/image_1704619335169_0.png)
- Some idempotent actions may include some transactions here to restore the atomicity.
- DAG supports a wide range of jobs at the cost of sacrificing the simplicity brought by map reduce.
- **Optimization for AI**:
	- Ease for gradient computation.
	- Ease for graph fusion to simplify the computation.
- ### Data Parallelism
- ![image.png](../assets/image_1704621566078_0.png)
- Parameter Server: On parameter update, send the updated parameter to the server, and send the parameters back to processors.
- Decentralized Allreduce: Concurrently send the parameter to its previous and after P rounds, the node can collect all parameters from its counterparts across the net.
- Ring allreduce: partition only send one partition at a time and combine and send the combined one to another. In the end, exchange the partitions of P and collect them together.
- ### Model Parallelism
- #### Layer Parallelism
- Microbatching: the acceleration ratio  $p-1/m$
- $p$ is partitioning factor and can be decreased by removing device. (Parameters are too large we can not reduce it).
- Maybe increasing m, no since large batch size will significantly affect the training performance.
- #### Tensor Parallelism
- We partitioned the weight into several parts and calculate part.
- ![image.png](../assets/image_1704622878226_0.png)
- ![image.png](../assets/image_1704622886575_0.png)
- ![image.png](../assets/image_1704622895140_0.png)
- ![image.png](../assets/image_1704622931157_0.png)
- ### Asynchronous vs Synchronous
- Up to now, we have only discussed synchronous update. We have a barrier when trying to update the model.
- However, in some cases where the model does not share the same training dataset, the asynchronous will do better.
- ### Modern Network
- ### DPDK
- Works in the userspace, do not need the kernel tcp/ip stack.
- However, we need to rework every protocol needed for communication.
- ![image.png](../assets/image_1704623183790_0.png)
-
- ### RDMA
- We have three queues. Completion Queue, Send Queue and Receive Queue.
- All queues reside in the memory and DMA and MMIO step in to directly copy the data into the memory bypassing the kernel.
- ![image.png](../assets/image_1704623222396_0.png)
- ![image.png](../assets/image_1704623231853_0.png)
- ![image.png](../assets/image_1704623256742_0.png)
- ---
- ## Security
- ### Difference between fault tolerance
- Result of the fault is too much.
- Failures due to attack might be highly correlated not independently.
- ### Policy
- Decide the authentication model and method.
- ### Threat Model
- Assumptions of the components that the adversary cannot control
- ---
- ## Password Authentication Attack Model
- ### Principle
- Hashing the password for fixed length hash
  logseq.order-list-type:: number
- Specific Password for each site
  logseq.order-list-type:: number
- Turn offline into online attack --- compulsorily let the attacker need to turn itself to online to be the man in the middle.
  logseq.order-list-type:: number
- One-time Password --- use hashing for 100 times generate a chain of password
  logseq.order-list-type:: number
- logseq.order-list-type:: number
-